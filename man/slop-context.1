.nh
.TH "SLOP" "1" "Jul 2025" "Slop CLI" ""

.SH NAME
slop-context - Manage persistent context for the current directory


.SH SYNOPSIS
\fBslop context [flags]\fP


.SH DESCRIPTION
Manage persistent file context using .slop/context manifest files. Searches for manifest in current directory and parent directories.


.SH OPTIONS
\fB-h\fP, \fB--help\fP[=false]
	help for context


.SH OPTIONS INHERITED FROM PARENT COMMANDS
\fB--config\fP=""
	Path to the config file

.PP
\fB--context\fP=[]
	Path to context file(s)

.PP
\fB-D\fP, \fB--debug\fP[=false]
	Enable detailed debug logging

.PP
\fB-d\fP, \fB--deep\fP[=false]
	Use deep/reasoning model

.PP
\fB-f\fP, \fB--fast\fP[=false]
	Use fast/lightweight model

.PP
\fB-i\fP, \fB--ignore-context\fP[=false]
	Ignore project context for this command

.PP
\fB--json\fP[=false]
	Format response as JSON

.PP
\fB--jsonl\fP[=false]
	Format response as JSONL (JSON Lines)

.PP
\fB-l\fP, \fB--local\fP[=false]
	Use local LLM

.PP
\fB--max-retries\fP=1
	Maximum number of retry attempts for failed requests (max: 5)

.PP
\fB--max-tokens\fP=2048
	Maximum number of tokens for LLM responses

.PP
\fB--md\fP[=false]
	Format response as Markdown

.PP
\fB-r\fP, \fB--remote\fP[=false]
	Use remote LLM

.PP
\fB--seed\fP=0
	Random seed for deterministic LLM outputs (0 = no seed)

.PP
\fB--stop-sequences\fP=["
",###]
	Stop sequences for LLM responses

.PP
\fB--system\fP=""
	The system prompt

.PP
\fB--temperature\fP=0.7
	Temperature for LLM responses

.PP
\fB--timeout\fP=60
	Timeout in seconds for LLM requests

.PP
\fB--top-p\fP=1
	Top P sampling for LLM responses

.PP
\fB-v\fP, \fB--verbose\fP[=false]
	Display LLM parameters in formatted table

.PP
\fB--xml\fP[=false]
	Format response as XML

.PP
\fB--yaml\fP[=false]
	Format response as YAML


.SH SEE ALSO
\fBslop(1)\fP, \fBslop-context-add(1)\fP, \fBslop-context-clear(1)\fP, \fBslop-context-list(1)\fP


.SH HISTORY
20-Jul-2025 Auto generated by spf13/cobra
