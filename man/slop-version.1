.nh
.TH "SLOP" "1" "Jun 2025" "Slop CLI" ""

.SH NAME
slop-version - Show version information


.SH SYNOPSIS
\fBslop version [flags]\fP


.SH DESCRIPTION
Display the current version of slop.


.SH OPTIONS
\fB-h\fP, \fB--help\fP[=false]
	help for version


.SH OPTIONS INHERITED FROM PARENT COMMANDS
\fB--config\fP=""
	Path to the config file

.PP
\fB--context\fP=[]
	Paths to context files

.PP
\fB-D\fP, \fB--debug\fP[=false]
	Enable detailed debug logging

.PP
\fB-d\fP, \fB--deep\fP[=false]
	Use deep/reasoning model

.PP
\fB-f\fP, \fB--fast\fP[=false]
	Use fast/lightweight model

.PP
\fB--json\fP[=false]
	Format response as JSON

.PP
\fB-l\fP, \fB--local\fP[=false]
	Use local LLM

.PP
\fB--max-retries\fP=1
	Maximum number of retry attempts for failed requests (max: 5)

.PP
\fB--max-tokens\fP=2048
	Maximum number of tokens for LLM responses

.PP
\fB--md\fP[=false]
	Format response as Markdown

.PP
\fB-r\fP, \fB--remote\fP[=false]
	Use remote LLM

.PP
\fB--seed\fP=0
	Random seed for deterministic LLM outputs (0 = no seed)

.PP
\fB--stop-sequences\fP=["
",###]
	Stop sequences for LLM responses

.PP
\fB--system\fP=""
	The system prompt

.PP
\fB--temperature\fP=0.7
	Temperature for LLM responses

.PP
\fB--timeout\fP=60
	Timeout in seconds for LLM requests

.PP
\fB--top-p\fP=1
	Top P sampling for LLM responses

.PP
\fB-v\fP, \fB--verbose\fP[=false]
	Display LLM parameters in formatted table

.PP
\fB--xml\fP[=false]
	Format response as XML

.PP
\fB--yaml\fP[=false]
	Format response as YAML


.SH SEE ALSO
\fBslop(1)\fP


.SH HISTORY
29-Jun-2025 Auto generated by spf13/cobra
