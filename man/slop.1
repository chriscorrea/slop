.nh
.TH "SLOP" "1" "Jul 2025" "Slop CLI" ""

.SH NAME
slop - A CLI tool for interacting with LLMs


.SH SYNOPSIS
\fBslop [flags]\fP


.SH DESCRIPTION
Slop brings large language models to your command line. It is inspired by the idea that language models work best as composable language operators


.SH OPTIONS
\fB--config\fP=""
	Path to the config file

.PP
\fB--context\fP=[]
	Path to context file(s)

.PP
\fB-D\fP, \fB--debug\fP[=false]
	Enable detailed debug logging

.PP
\fB-d\fP, \fB--deep\fP[=false]
	Use deep/reasoning model

.PP
\fB-f\fP, \fB--fast\fP[=false]
	Use fast/lightweight model

.PP
\fB-h\fP, \fB--help\fP[=false]
	help for slop

.PP
\fB-i\fP, \fB--ignore-context\fP[=false]
	Ignore project context for this command

.PP
\fB--json\fP[=false]
	Format response as JSON

.PP
\fB--jsonl\fP[=false]
	Format response as JSONL (JSON Lines)

.PP
\fB-l\fP, \fB--local\fP[=false]
	Use local LLM

.PP
\fB--max-retries\fP=1
	Maximum number of retry attempts for failed requests (max: 5)

.PP
\fB--max-tokens\fP=2048
	Maximum number of tokens for LLM responses

.PP
\fB--md\fP[=false]
	Format response as Markdown

.PP
\fB-r\fP, \fB--remote\fP[=false]
	Use remote LLM

.PP
\fB--seed\fP=0
	Random seed for deterministic LLM outputs (0 = no seed)

.PP
\fB--stop-sequences\fP=["
",###]
	Stop sequences for LLM responses

.PP
\fB--system\fP=""
	The system prompt

.PP
\fB--temperature\fP=0.7
	Temperature for LLM responses

.PP
\fB--timeout\fP=60
	Timeout in seconds for LLM requests

.PP
\fB--top-p\fP=1
	Top P sampling for LLM responses

.PP
\fB-v\fP, \fB--verbose\fP[=false]
	Display LLM parameters in formatted table

.PP
\fB--xml\fP[=false]
	Format response as XML

.PP
\fB--yaml\fP[=false]
	Format response as YAML


.SH SEE ALSO
\fBslop-completion(1)\fP, \fBslop-config(1)\fP, \fBslop-context(1)\fP, \fBslop-help-command(1)\fP, \fBslop-init(1)\fP, \fBslop-list(1)\fP, \fBslop-version(1)\fP


.SH HISTORY
20-Jul-2025 Auto generated by spf13/cobra
